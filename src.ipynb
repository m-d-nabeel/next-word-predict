{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d projjal1/human-conversation-training-data\n",
    "# !unzip *.zip\n",
    "# !rm -rf *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "with open(\"human_chat.txt\", \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "corpus = corpus.replace(\"Human 1:\", \"\")\n",
    "corpus = corpus.replace(\"Human 2:\", \"\")\n",
    "corpus = corpus.lower()\n",
    "corpus = contractions.fix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' hi!',\n",
       " 'what is your favorite holiday?',\n",
       " 'one where i get to meet lots of different people.',\n",
       " 'what was the most number of people you have ever met during a holiday?',\n",
       " 'hard to keep a count.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.text import sent_tokenize\n",
    "\n",
    "cwd = os.curdir\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=cwd)\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'what is your favorite holiday',\n",
       " 'one where i get to meet lots of different people',\n",
       " 'what was the most number of people you have ever met during a holiday',\n",
       " 'hard to keep a count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentences = [re.sub(r\"[^a-zA-Z ]\", \"\", sentence).strip() for sentence in sentences]\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2723"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = len(tokenizer.word_index)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max([len(sentence.split()) for sentence in sentences])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_sequence = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence.append(token_list[: i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for sequence in n_gram_sequence:\n",
    "    X.append(sequence[:-1])\n",
    "    y.append(sequence[-1])\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_length + 1, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  6]]\n",
      "[6, 27]\n"
     ]
    }
   ],
   "source": [
    "print(X[:2])\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(y, num_classes=unique_words + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 108ms/step - accuracy: 0.0345 - loss: 6.5986\n",
      "Epoch 2/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 103ms/step - accuracy: 0.1053 - loss: 5.6492\n",
      "Epoch 3/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 114ms/step - accuracy: 0.1349 - loss: 5.2064\n",
      "Epoch 4/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 110ms/step - accuracy: 0.1609 - loss: 4.8348\n",
      "Epoch 5/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 113ms/step - accuracy: 0.1841 - loss: 4.5078\n",
      "Epoch 6/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 109ms/step - accuracy: 0.2008 - loss: 4.2404\n",
      "Epoch 7/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 109ms/step - accuracy: 0.2180 - loss: 3.9591\n",
      "Epoch 8/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 103ms/step - accuracy: 0.2532 - loss: 3.6544\n",
      "Epoch 9/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 101ms/step - accuracy: 0.2945 - loss: 3.4197\n",
      "Epoch 10/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 95ms/step - accuracy: 0.3319 - loss: 3.1889\n",
      "Epoch 11/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 112ms/step - accuracy: 0.3682 - loss: 2.9690\n",
      "Epoch 12/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 120ms/step - accuracy: 0.4095 - loss: 2.7492\n",
      "Epoch 13/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 119ms/step - accuracy: 0.4488 - loss: 2.5543\n",
      "Epoch 14/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 116ms/step - accuracy: 0.4820 - loss: 2.3864\n",
      "Epoch 15/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 112ms/step - accuracy: 0.5176 - loss: 2.2082\n",
      "Epoch 16/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 105ms/step - accuracy: 0.5527 - loss: 2.0614\n",
      "Epoch 17/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 98ms/step - accuracy: 0.5785 - loss: 1.9372\n",
      "Epoch 18/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 98ms/step - accuracy: 0.6139 - loss: 1.7774\n",
      "Epoch 19/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 99ms/step - accuracy: 0.6357 - loss: 1.6706\n",
      "Epoch 20/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 113ms/step - accuracy: 0.6733 - loss: 1.5503\n",
      "Epoch 21/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 119ms/step - accuracy: 0.6887 - loss: 1.4393\n",
      "Epoch 22/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 117ms/step - accuracy: 0.7103 - loss: 1.3578\n",
      "Epoch 23/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 106ms/step - accuracy: 0.7291 - loss: 1.2714\n",
      "Epoch 24/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 103ms/step - accuracy: 0.7435 - loss: 1.1840\n",
      "Epoch 25/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 111ms/step - accuracy: 0.7596 - loss: 1.1258\n",
      "Epoch 26/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 109ms/step - accuracy: 0.7836 - loss: 1.0419\n",
      "Epoch 27/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 109ms/step - accuracy: 0.7815 - loss: 1.0078\n",
      "Epoch 28/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 108ms/step - accuracy: 0.7959 - loss: 0.9507\n",
      "Epoch 29/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 113ms/step - accuracy: 0.8073 - loss: 0.8932\n",
      "Epoch 30/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 122ms/step - accuracy: 0.8155 - loss: 0.8391\n",
      "Epoch 31/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 109ms/step - accuracy: 0.8227 - loss: 0.8021\n",
      "Epoch 32/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 108ms/step - accuracy: 0.8272 - loss: 0.7791\n",
      "Epoch 33/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8323 - loss: 0.7296\n",
      "Epoch 34/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8374 - loss: 0.7145\n",
      "Epoch 35/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 90ms/step - accuracy: 0.8336 - loss: 0.7078\n",
      "Epoch 36/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8456 - loss: 0.6581\n",
      "Epoch 37/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8423 - loss: 0.6471\n",
      "Epoch 38/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 90ms/step - accuracy: 0.8460 - loss: 0.6290\n",
      "Epoch 39/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 90ms/step - accuracy: 0.8481 - loss: 0.6049\n",
      "Epoch 40/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 90ms/step - accuracy: 0.8486 - loss: 0.5962\n",
      "Epoch 41/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 90ms/step - accuracy: 0.8544 - loss: 0.5733\n",
      "Epoch 42/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 91ms/step - accuracy: 0.8489 - loss: 0.5757\n",
      "Epoch 43/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8495 - loss: 0.5656\n",
      "Epoch 44/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8534 - loss: 0.5529\n",
      "Epoch 45/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8508 - loss: 0.5536\n",
      "Epoch 46/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8530 - loss: 0.5456\n",
      "Epoch 47/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8499 - loss: 0.5390\n",
      "Epoch 48/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8534 - loss: 0.5353\n",
      "Epoch 49/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8514 - loss: 0.5283\n",
      "Epoch 50/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.8513 - loss: 0.5301\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(unique_words + 1, 100))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(unique_words + 1, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, y, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_next_words(input_text):\n",
    "  token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "  token_list = pad_sequences([token_list], maxlen=max_length+1, padding=\"pre\")\n",
    "\n",
    "  predicted_token = model.predict(token_list)\n",
    "  predicted_token = np.argsort(predicted_token[0])[-3:]\n",
    "  predicted_words = []\n",
    "  for token in predicted_token:\n",
    "    predicted_words.append(tokenizer.index_word[token])\n",
    "  return predicted_words[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['am', 'have', 'think']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_words(\"i \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "i have been super busy so i have not study much about\n"
     ]
    }
   ],
   "source": [
    "out = \"i have\"\n",
    "for i in range(10):\n",
    "    out += \" \" + predict_next_words(out)[0]\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"predict_next_words.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(tokenizer, \"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "what\n",
      "['is', 'are', 'about']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = joblib.load(\"tokenizer\")\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"predict_next_words.keras\")\n",
    "\n",
    "# Function to predict next words\n",
    "def predict_next_words(input_text):\n",
    "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_length+1, padding=\"pre\")\n",
    "\n",
    "    predicted_token = model.predict(token_list)\n",
    "    predicted_token = np.argsort(predicted_token[0])[-3:]\n",
    "    predicted_words = []\n",
    "    for token in predicted_token:\n",
    "        predicted_words.append(tokenizer.index_word[token])\n",
    "    print(input_text)\n",
    "    return predicted_words[::-1]\n",
    "\n",
    "# Example usage\n",
    "predicted_words = predict_next_words(\"what\")\n",
    "print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quora-duplicate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
